# Crawl4AI + MCP Pipeline

ğŸš€ A complete, production-grade system for smart web crawling, AI-driven content processing, and structured data storage â€” with zero architecture guesswork!

---

## ğŸ“¦ Project Structure

```
crawl4ai-mcp/
â”œâ”€ 01_Crawl4AI_Scraper/          # Python scraper + helper modules
â”‚   â”œâ”€ crawler.py
â”‚   â”œâ”€ parser.py
â”‚   â””â”€ requirements.txt
â”œâ”€ 02_MCP_Modules/               # AI JSON modules (M1-M6)
â”‚   â”œâ”€ m1-extract-urls.json
â”‚   â”œâ”€ m2-fetch-content.json
â”‚   â”œâ”€ m3-clean-html.json
â”‚   â”œâ”€ m4-summarize.json
â”‚   â”œâ”€ m5-extract-entities.json
â”‚   â””â”€ m6-format-output.json
â”œâ”€ 03_Integration/               # n8n workflow export & connector
â”‚   â”œâ”€ n8n_workflow.json
â”‚   â””â”€ connector.js
â”œâ”€ 04_Logging/                   # Centralized logger
â”‚   â””â”€ logger.py
â”œâ”€ config/                       # Configuration and environment settings
â”‚   â”œâ”€ config.example.json
â”‚   â””â”€ .env
â””â”€ README.md                     # Project guide
```

---

## âš¡ Quick Start

### 1. Clone the repository (if needed)
```bash
git clone https://github.com/your-repo/crawl4ai-mcp.git
cd crawl4ai-mcp
```

### 2. Setup Python environment
```bash
cd 01_Crawl4AI_Scraper/
pip install -r requirements.txt
playwright install
```

---

### 3. Configure your project
- Copy `config/config.example.json` to `config/config.json`.
- Update your real API keys, URLs, rate limits.
- Fill `.env` file with your secrets (OpenAI, SerpAPI, Google Sheets, etc.).

---

### 4. Run the crawler manually (optional test)
```bash
python crawler.py
```
âœ… Output will be saved in `/output/cleaned_pages.json`.

---

### 5. Setup and run n8n

Install and start n8n:
```bash
npm install -g n8n
n8n start
```

- Open browser: [http://localhost:5678](http://localhost:5678)
- Import `03_Integration/n8n_workflow.json`
- Connect nodes and add your credentials (Google Sheets, OpenAI, etc.)
- Trigger the workflow!

---

## ğŸ“ˆ Advanced Setup

- Setup Prometheus and Grafana for monitoring.
- Deploy using Docker and Kubernetes if scaling is needed.
- Migrate storage from Google Sheets â” Azure CosmosDB â” Pinecone/Weaviate (semantic search).

---

## ğŸ›¡ï¸ Security Best Practices

- Store your `.env` securely.
- Never commit API keys to GitHub.
- Use Vault / AWS Secrets Manager in production.

---

## ğŸ“‘ Documentation Links
- [n8n Documentation](https://docs.n8n.io/)
- [Playwright Python](https://playwright.dev/python/)
- [OpenAI API Docs](https://platform.openai.com/docs/)
- [Pinecone Docs](https://docs.pinecone.io/)

---

## ğŸ‘¨â€ğŸ’» About
Built with â¤ï¸ for intelligent, scalable, AI-powered web crawling and data extraction.
